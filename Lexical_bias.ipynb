{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\en750\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from sentence_transformers import util\n",
    "import helper_functions as hp\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "class CustomDatasetTriplet(Dataset):\n",
    "    def __init__(self, dataset):#store the dataset in memory for faster acess\n",
    "        self.edit_sentences = [val[4] for val in dataset]\n",
    "        self.paraphrase_sentences = [val[5] for val in dataset]\n",
    "        self.neigbhourhood_sentences = [val[6] for val in dataset]\n",
    "        self.edit_vectors = [val[0] for val in dataset]\n",
    "        self.paraphrase_vectors = [val[1] for val in dataset]\n",
    "        self.neighbourhood_vectors = [val[2] for val in dataset]\n",
    "    def __len__(self):\n",
    "        return len(self.edit_vectors)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        emb1 = torch.tensor(self.edit_vectors[index], dtype=torch.float32)\n",
    "        emb2 = torch.tensor(self.paraphrase_vectors[index], dtype=torch.float32)\n",
    "        emb3 = torch.tensor(self.neighbourhood_vectors[index], dtype=torch.float32)\n",
    "        sent1 = self.edit_sentences[index]\n",
    "        sent2 = self.paraphrase_sentences[index]\n",
    "        sent3 = self.neigbhourhood_sentences[index]\n",
    "        \n",
    "        return emb1, emb2, emb3, sent1, sent2, sent3\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):#do not duplicate data approach\n",
    "    def __init__(self, dataset, device):\n",
    "        self.dataset = np.array(dataset, dtype=object)\n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def total_indexes(self):\n",
    "        return np.unique(self.dataset[:, 3])\n",
    "\n",
    "    def get_row_indexes(self, target_sample_index):\n",
    "        return np.where(self.dataset[:, 3] == target_sample_index)[0]\n",
    "\n",
    "    def get_samples_at_data_index(self, target_sample_index):\n",
    "        row_indexes = np.where(self.dataset[:, 3] == target_sample_index)[0]\n",
    "        embs_edit, embs_paraphrase, embs_neighbour, row_indexes, sents_edit, sents_paraphrase, sents_neigbhour = [], [], [], [], [], [], []\n",
    "        \n",
    "        for index in row_indexes:\n",
    "            embs_edit.append(torch.tensor(self.dataset[index][0], dtype=torch.float32).to(self.device))\n",
    "            embs_paraphrase.append(torch.tensor(self.dataset[index][1], dtype=torch.float32).to(self.device))\n",
    "            embs_neighbour.append(torch.tensor(self.dataset[index][2], dtype=torch.float32).to(self.device))\n",
    "            row_indexes.append(self.dataset[index][3])\n",
    "            sents_edit.append(self.dataset[index][4])\n",
    "            sents_paraphrase.append(self.dataset[index][5])\n",
    "            sents_neigbhour.append(self.dataset[index][6])\n",
    "        \n",
    "        return embs_edit, embs_paraphrase, embs_neighbour, row_indexes, sents_edit, sents_paraphrase, sents_neigbhour\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        emb_edit = torch.tensor(self.dataset[index][0], dtype=torch.float32).to(self.device)\n",
    "        emb_paraphrase = torch.tensor(self.dataset[index][1], dtype=torch.float32).to(self.device)\n",
    "        emb_neighbour = torch.tensor(self.dataset[index][2], dtype=torch.float32).to(self.device)\n",
    "        row_index = self.dataset[index][3]\n",
    "        sent_edit = self.dataset[index][4]\n",
    "        sent_paraphrase = self.dataset[index][5]\n",
    "        sent_neigbhour = self.dataset[index][6]\n",
    "        #print(self.dataset[index][2])\n",
    "        \n",
    "        return emb_edit, emb_paraphrase, emb_neighbour, row_index, sent_edit, sent_paraphrase, sent_neigbhour\n",
    "\n",
    "\n",
    "def get_data_loader(dataset_paired, batch_size=2, shuffle=True, device=\"cpu\"):\n",
    "    dataset_pt = CustomDataset(dataset_paired, device)\n",
    "    data_loader = DataLoader(dataset_pt, batch_size=batch_size, shuffle=shuffle)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_tripletloss(dataset,mode=0,label_reversal=False):\n",
    "    \"\"\"\n",
    "    The dataset is created in a set format for both test and train sets.\n",
    "    The first value will always be the edit vector, the second a paraphrase vector and the thirt a neighbour vector.\n",
    "    This is followed by the row index, the text prompts for edit, praraphrase and neighbours\n",
    "\n",
    "    While using the test set the test paraphrase will be in all the rows but need to evaluated only once and can be ignored there after.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "        \n",
    "    if(label_reversal==True):\n",
    "        paraphrase=0\n",
    "        neightbour=1\n",
    "    else:\n",
    "        paraphrase=1\n",
    "        neightbour=0\n",
    "\n",
    "    dataset_paired_train=[]\n",
    "    dataset_paired_test=[]\n",
    "   \n",
    "    for row_index,row in enumerate(dataset):#iterate over the dataset\n",
    "        index_control_neighbourhood=len(row[\"vectors_neighborhood_prompts_high_sim\"])-1#number of entries in the neighbourhood \n",
    "        # print(index_control_neighbourhood)\n",
    "        num_elements_to_select = min(3, len(row[\"openai_usable_paraphrases_embeddings\"]))#add 3 max open ai paraphrases\n",
    "        #with openai paraphrases set to 3 and 1 paraphrase from the dataset there are 4 elements, total neighbourhood elements are 5\n",
    "        #I have made the code such that there is sampling for paraphrase for 5th element based on random sampling, othere wise you can just use 4 elements from neighbourhood\n",
    "        sampled_indices, sampled_elements = zip(*random.sample(list(enumerate(row[\"openai_usable_paraphrases_embeddings\"])), num_elements_to_select))# sample and get indexes\n",
    "        for index, vector_openai in zip(sampled_indices, sampled_elements):\n",
    "            dataset_paired_train.append([row[\"vector_edited_prompt\"],vector_openai,row[\"vectors_neighborhood_prompts_high_sim\"][abs(index_control_neighbourhood)],row_index,\n",
    "                                        row[\"edited_prompt\"][0],row[\"openai_usable_paraphrases\"][index],row[\"neighborhood_prompts_high_sim\"][abs(index_control_neighbourhood)]])\n",
    "            index_control_neighbourhood=index_control_neighbourhood-1\n",
    "\n",
    "        dataset_paired_train.append([row[\"vector_edited_prompt\"],row[\"vector_edited_prompt_paraphrases_processed\"],row[\"vectors_neighborhood_prompts_high_sim\"][abs(index_control_neighbourhood)],row_index,\n",
    "                                        row[\"edited_prompt\"][0],row[\"openai_usable_paraphrases\"][index],row[\"neighborhood_prompts_high_sim\"][abs(index_control_neighbourhood)]])\n",
    "        index_control_neighbourhood=index_control_neighbourhood-1\n",
    "        #at this point the index is zero with one neigbour not being used. you can add it if you want to.\n",
    "\n",
    "            \n",
    "        #test set\n",
    "        for index,vector in enumerate(row[\"vectors_neighborhood_prompts_low_sim\"]):\n",
    "            dataset_paired_test.append([row[\"vector_edited_prompt\"],row[\"vector_edited_prompt_paraphrases_processed_testing\"],vector,row_index,\n",
    "                                        row[\"edited_prompt\"][0],row[\"edited_prompt_paraphrases_processed_testing\"],row[\"neighborhood_prompts_low_sim\"][index]])\n",
    "      \n",
    "            \n",
    "\n",
    "    return  dataset_paired_train,dataset_paired_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the dataloaders\n",
    "import json,linecache\n",
    "def read_dataset_reduced(file_path_read_dataset: str,data_size):\n",
    "    dataset=[]\n",
    "    values_list = list(range(1, data_size+1))\n",
    "    for index,number in enumerate(values_list):\n",
    "\n",
    "        try:\n",
    "            data_entry = json.loads(linecache.getline(file_path_read_dataset, number).strip())\n",
    "            dataset.append(data_entry)\n",
    "        except Exception as e:\n",
    "            print(index)\n",
    "            print(e)\n",
    "    return dataset\n",
    "file_path_dataset=\"counterfact_test_2_lama_merged.jsonl\"\n",
    "num_samples=4999\n",
    "dataset=read_dataset_reduced(file_path_dataset,data_size=num_samples) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output vector length: 4096\n"
     ]
    }
   ],
   "source": [
    "dataset_paired_train,dataset_paired_test=create_dataset_tripletloss(dataset)\n",
    "input_dim = len(dataset_paired_test[0][0][0])  \n",
    "print(f\"output vector length: {input_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset_paired_train, dataset_paired_test = create_dataset_tripletloss(dataset, mode=0, label_reversal=False)\n",
    "train_loader = get_data_loader(dataset_paired_train, batch_size=batch_size, device=device)\n",
    "test_loader = get_data_loader(dataset_paired_test, batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def calculate_distances(anchor, positive, negative):\n",
    "    cos_pos = float(cosine_similarity(anchor.cpu().numpy().reshape(1, -1), \n",
    "                                positive.cpu().numpy().reshape(1, -1))[0][0])\n",
    "    cos_neg = float(cosine_similarity(anchor.cpu().numpy().reshape(1, -1), \n",
    "                                negative.cpu().numpy().reshape(1, -1))[0][0])\n",
    "    \n",
    "    dist_pos = float(F.pairwise_distance(anchor.unsqueeze(0), positive.unsqueeze(0)).item())\n",
    "    dist_neg = float(F.pairwise_distance(anchor.unsqueeze(0), negative.unsqueeze(0)).item())\n",
    "    \n",
    "    return cos_pos, cos_neg, dist_pos, dist_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_threshold(emb_edit, emb_para):\n",
    "    \n",
    "    dist = torch.dist(emb_edit, emb_para).item()\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thresholds saved for 4999 edit vectors\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "threshold_map = defaultdict(list)\n",
    "\n",
    "for edit_vector, paraphrase_vector, neighbor_vector, row_index, _, _, _ in train_loader:\n",
    "\n",
    "    for i, idx in enumerate(row_index):\n",
    "        idx = int(idx.item())  \n",
    "        threshold = compute_threshold(edit_vector[i], paraphrase_vector[i])\n",
    "\n",
    "        threshold_map[idx].append(threshold)\n",
    "\n",
    "\n",
    "final_threshold_map = {str(k): max(v) for k, v in threshold_map.items()}\n",
    "\n",
    "\n",
    "with open(\"Lexical_threashold.json\", \"w\") as f:\n",
    "    json.dump(final_threshold_map, f, indent=4)\n",
    "\n",
    "print(f\"Thresholds saved for {len(final_threshold_map)} edit vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loca_success: 23955\n",
      "gen_success: 20820\n",
      "Total: 24995\n",
      "Correct: 20820\n",
      "Generalization Rate: 83.30%\n",
      "Locality Rate: 95.84%\n",
      "Incorrect Predictions Saved to 'Lexical bias outcome.json'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with open(\"Lexical_Threashold.json\", \"r\") as f:\n",
    "    threshold_map = json.load(f)\n",
    "\n",
    "threshold_map = {str(k): v for k, v in threshold_map.items()}\n",
    "\n",
    "def predict_label(edit_vector_test, paraphrase_vector_test, neighbor_vector_test, threshold, margin=0.1):\n",
    "    anchor = edit_vector_test\n",
    "    positive = paraphrase_vector_test\n",
    "    negative = neighbor_vector_test\n",
    "\n",
    "    dist_para = torch.dist(anchor, positive).item()\n",
    "    dist_neigh = torch.dist(anchor, negative).item()\n",
    "\n",
    "\n",
    "    if dist_para < threshold:\n",
    "        if dist_neigh > threshold:\n",
    "            return 1, dist_para, dist_neigh, True, True\n",
    "        else:\n",
    "            return 1, dist_para, dist_neigh, True, False\n",
    "    else:\n",
    "        if dist_neigh > threshold:\n",
    "            return 0, dist_para, dist_neigh, False, True\n",
    "        else:\n",
    "            return 0, dist_para, dist_neigh, False, False\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "Generation_success = 0\n",
    "locality_success = 0\n",
    "incorrect_predictions = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for edit_vector, paraphrase_vector, neighbor_vector, row_index, edit_sentence, paraphrase_sentence, neighbor_sentence in test_loader:\n",
    "\n",
    "        if isinstance(row_index, torch.Tensor):\n",
    "            row_index = [int(idx.item()) for idx in row_index]\n",
    "\n",
    "        for i, index in enumerate(row_index):\n",
    "            threshold = threshold_map.get(str(index), 1.0)\n",
    "\n",
    "            predicted, dist_para, dist_neigh, gen_success, loc_success = predict_label(\n",
    "                edit_vector[i], \n",
    "                paraphrase_vector[i], \n",
    "                neighbor_vector[i], \n",
    "                threshold,\n",
    "            )\n",
    "\n",
    "            if gen_success:\n",
    "                Generation_success += 1\n",
    "            \n",
    "            if loc_success:\n",
    "                locality_success += 1\n",
    "\n",
    "            if predicted == 1:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect_predictions.append({\n",
    "                    \"edit_sentence\": edit_sentence[i],\n",
    "                    \"paraphrase_sentence\": paraphrase_sentence[i],\n",
    "                    \"neighbor_sentence\": neighbor_sentence[i],\n",
    "                    \"distance_paraphrase\": dist_para,\n",
    "                    \"distance_neighbor\": dist_neigh,\n",
    "                    \"threshold\": threshold\n",
    "                })\n",
    "\n",
    "            total += 1\n",
    "\n",
    "with open(\"Lexical bias outcome.json\", \"w\") as f:\n",
    "    json.dump(incorrect_predictions, f, indent=4)\n",
    "\n",
    "print(f\"loca_success: {locality_success}\")\n",
    "print(f\"gen_success: {Generation_success}\")\n",
    "\n",
    "Generation_rate = Generation_success / total\n",
    "locality_rate = locality_success / total\n",
    "\n",
    "print(f\"Total: {total}\")\n",
    "print(f\"Correct: {correct}\")\n",
    "print(f\"Generalization Rate: {Generation_rate:.2%}\")\n",
    "print(f\"Locality Rate: {locality_rate:.2%}\")\n",
    "print(f\"Incorrect Predictions Saved to 'Lexical bias outcome.json'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
