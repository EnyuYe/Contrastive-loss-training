{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\en750\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from sentence_transformers import util\n",
    "import helper_functions as hp\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "#from imports import Dataset,torch,np,random,DataLoader,util\n",
    "# DATASET CLASSES PYTORCH \n",
    "# import helper_functions as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,dataset,openai_vectors_dict, edit_vectors_dict, neighbourhood_vectors_dict, paraphrase_vectors_dict,device):\n",
    "        self.dataset=np.array(dataset,dtype=object)\n",
    "        self.openai_vectors_dict=openai_vectors_dict\n",
    "        self.edit_vectors_dict=edit_vectors_dict\n",
    "        self.neighbourhood_vectors_dict=neighbourhood_vectors_dict\n",
    "        self.paraphrase_vectors_dict=paraphrase_vectors_dict\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def total_indexes(self):\n",
    "        # print(self.dataset[0][2:])\n",
    "        return np.unique(self.dataset[:, 3])\n",
    "\n",
    "    def get_row_indexes(self,target_sample_index):\n",
    "        return np.where(self.dataset[:, 3] == target_sample_index)[0]\n",
    "\n",
    "    def get_samples_at_data_index(self,target_sample_index):\n",
    "        row_indexes = np.where(self.dataset[:, 3] == target_sample_index)[0]\n",
    "        emb1=[]\n",
    "        emb2=[]\n",
    "        label=[]\n",
    "        row_index=[]\n",
    "        sent1=[]\n",
    "        sent2=[]\n",
    "        for index in row_indexes:\n",
    "        \n",
    "          emb1.append(hp.to_tensor(self.dataset[index][0][0]))\n",
    "          emb2.append(hp.to_tensor(self.dataset[index][1][0]))\n",
    "          label.append(hp.to_tensor(self.dataset[index][2]))\n",
    "          print(f\"label: {label[index]}\")\n",
    "          row_index.append(hp.to_tensor(self.dataset[index][3]))\n",
    "          sent1.append(self.dataset[index][4])\n",
    "          sent2.append(self.dataset[index][5])\n",
    "        return emb1.to(self.device), emb2.to(self.device), label.to(self.device),row_index, sent1, sent2\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_row=self.dataset[index]\n",
    "        # print(\"aloha\",data_row,data_row[0])\n",
    "        if(data_row[-2]==0):#open ai paraphrase\n",
    "            emb1 = hp.to_tensor(self.edit_vectors_dict[data_row[0]]).to(self.device)#, dtype=torch.float)\n",
    "            emb2 = hp.to_tensor(self.openai_vectors_dict[data_row[0]][data_row[1]]).to(self.device)#, dtype=torch.float)\n",
    "            label = hp.to_tensor(self.dataset[index][2]).to(self.device)#, dtype=torch.long)\n",
    "            sample_index=self.dataset[index][3]\n",
    "            sent1=self.dataset[index][4]\n",
    "            sent2=self.dataset[index][5]\n",
    "            pair_type=self.dataset[index][6]\n",
    "            negative_sample_cntrl=self.dataset[index][7]\n",
    "            emb1_index=data_row[0]\n",
    "            emb2_index=data_row[1]\n",
    "\n",
    "        elif(data_row[-2]==1):#paraphrase\n",
    "            emb1 = hp.to_tensor(self.edit_vectors_dict[data_row[0]]).to(self.device)#, dtype=torch.float)\n",
    "            emb2 = hp.to_tensor(self.paraphrase_vectors_dict[data_row[0]]).to(self.device)#, dtype=torch.float)\n",
    "            label = hp.to_tensor(self.dataset[index][2]).to(self.device)#, dtype=torch.long)\n",
    "            sample_index=self.dataset[index][3]\n",
    "            sent1=self.dataset[index][4]\n",
    "            sent2=self.dataset[index][5]\n",
    "            pair_type=self.dataset[index][6]#neighbour,openai,paraphrase\n",
    "            negative_sample_cntrl=self.dataset[index][7]\n",
    "            emb1_index=data_row[0]#both should be the same\n",
    "            emb2_index=data_row[1]#both should be the same\n",
    "\n",
    "        else:#neighbour\n",
    "            emb1 = hp.to_tensor(self.edit_vectors_dict[data_row[1]]).to(self.device)#, dtype=torch.float)\n",
    "            emb2 = hp.to_tensor(self.neighbourhood_vectors_dict[data_row[1]][data_row[0]]).to(self.device)#, dtype=torch.float)\n",
    "            #print(f\"emb1.shape: {emb1.shape}, emb2.shape: {emb2.shape}\")\n",
    "            label = hp.to_tensor(self.dataset[index][2]).to(self.device)#, dtype=torch.long)\n",
    "            sample_index=self.dataset[index][3]\n",
    "            sent1=self.dataset[index][4]\n",
    "            sent2=self.dataset[index][5]\n",
    "            pair_type=self.dataset[index][6]\n",
    "            negative_sample_cntrl=self.dataset[index][7]\n",
    "            emb1_index=data_row[0]\n",
    "            emb2_index=data_row[1]\n",
    "\n",
    "        return emb1, emb2, label, sample_index, sent1, sent2, pair_type, emb1_index, emb2_index, negative_sample_cntrl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset_paired,openai_vectors_dict, edit_vectors_dict, neighbourhood_train_vectors_dict, paraphrase_train_vectors_dict,batch_size=8192,shuffle=True,device=\"cpu\"):\n",
    "  \"\"\"\n",
    "    dataset: dataset to be used\n",
    "    shuffle: dataset shuffle per iteration\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  dataset_pt=CustomDataset(dataset_paired,openai_vectors_dict, edit_vectors_dict, neighbourhood_train_vectors_dict, paraphrase_train_vectors_dict,device=device)\n",
    "  data_loader = DataLoader(dataset_pt, batch_size=batch_size, shuffle=shuffle)\n",
    "  return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_pairs(dataset,neightbour_control=0,label_reversal=False):\n",
    "    \"\"\"\n",
    "    The dataloader is designed to create pairwise samples. \n",
    "    Since an edit can have multyple paraphrases storing edit vector multiple times is redundant thus\n",
    "    all vectors are store in dictionary based on the sample index(order that sample appear in the dataset.\n",
    "\n",
    "    dataset_paired_train/dataset_paired_test=\n",
    "    [index1,index2,label,row_index,sentence1,sentence2,pair_type,control]\n",
    "        index1 and index2:\n",
    "            for edits index1 is always used and index1 == row_index since it is single value per sample\n",
    "            for paraphrases, open_ai_paraphrases index1,index2 is used, this is not row_index, but index of the\n",
    "                paraphrase in the list of paraphrases in a sample\n",
    "            for neighbours index2 is used, this is not row_index, but index of the\n",
    "                neighbour in the list of neighbours in a sample\n",
    "            for open_ai_paraphrases index2 is used this is not row_index, but index of the\n",
    "                open_ai_paraphrase in the list of open_ai_paraphrases in a sample\n",
    "                \n",
    "    Inputs:\n",
    "    dataset: path to dataset\n",
    "    neightbour_control=0# ignore\n",
    "    label_reversal=False#ignore\n",
    "    \"\"\"\n",
    "    paraphrase=1\n",
    "    neightbour=0\n",
    "\n",
    "\n",
    "\n",
    "    openai_vectors_dict={}\n",
    "    edit_vectors_dict={}\n",
    "    neighbourhood_train_vectors_dict={}\n",
    "    neighbourhood_test_vectors_dict={}\n",
    "    paraphrase_train_vectors_dict={}\n",
    "    paraphrase_test_vectors_dict={}\n",
    "\n",
    "    dataset_paired_train=[]\n",
    "    dataset_paired_test=[]\n",
    "    for row_index,row in enumerate(dataset):\n",
    "    #     # print(row[\"vector_edited_prompt\"][:5],row[\"edited_prompt\"][0])\n",
    "    #     # print(\"\\n\\n\")\n",
    "    #     # print(row[\"vector_edited_prompt\"][0]\n",
    "        \n",
    "        edit_vectors_dict[row_index]=row[\"vector_edited_prompt\"][0]\n",
    "        paraphrase_train_vectors_dict[row_index]=row[\"vector_edited_prompt_paraphrases_processed\"][0]\n",
    "        paraphrase_test_vectors_dict[row_index]=row[\"vector_edited_prompt_paraphrases_processed_testing\"][0]\n",
    "\n",
    "        num_elements_to_select = min(3, len(row[\"openai_usable_paraphrases_embeddings\"]))#add 5 max open ai paraphrases\n",
    "        # return  None, None, None, None, None, None, None, None\n",
    "\n",
    "        sampled_indices, sampled_elements = zip(*random.sample(list(enumerate(row[\"openai_usable_paraphrases_embeddings\"])), num_elements_to_select))# sample and get indexes\n",
    "        for index,vector in zip(sampled_indices, sampled_elements):#create postive label with edit vector\n",
    "            if(row_index not in openai_vectors_dict.keys()):\n",
    "                openai_vectors_dict[row_index]={}\n",
    "            openai_vectors_dict[row_index][index]=vector[0]\n",
    "            # print(vector[:5],row[\"openai_usable_paraphrases\"][index],\"openai\")\n",
    "            dataset_paired_train.append([row_index,index,paraphrase,row_index,\n",
    "                                    row[\"edited_prompt\"][0],row[\"openai_usable_paraphrases\"][index],0,0])\n",
    "\n",
    "\n",
    "        dataset_paired_train.append([row_index,row_index,paraphrase,row_index,\n",
    "                                    row[\"edited_prompt\"][0],row[\"edited_prompt_paraphrases_processed\"],1,1])\n",
    "        # print(row[\"edited_prompt_paraphrases_processed\"])\n",
    "        dataset_paired_test.append([row_index,row_index,paraphrase,row_index,\n",
    "                                    row[\"edited_prompt\"][0],row[\"edited_prompt_paraphrases_processed_testing\"],1,0])\n",
    "        \n",
    "        if(neightbour_control==0):\n",
    "            for index,vector in enumerate(row[\"vectors_neighborhood_prompts_high_sim\"]):\n",
    "                if(row_index not in neighbourhood_train_vectors_dict.keys()):\n",
    "                    neighbourhood_train_vectors_dict[row_index]={}\n",
    "                neighbourhood_train_vectors_dict[row_index][index]=vector[0]\n",
    "                dataset_paired_train.append([index,row_index,neightbour,row_index,\n",
    "                                        row[\"edited_prompt\"][0],row[\"neighborhood_prompts_high_sim\"][index],2,1])\n",
    "\n",
    "                # print(vector[:5],row[\"neighborhood_prompts_high_sim\"][index],\"high\")\n",
    "            for index,vector in enumerate(row[\"vectors_neighborhood_prompts_low_sim\"]):\n",
    "                if(row_index not in neighbourhood_test_vectors_dict.keys()):\n",
    "                    neighbourhood_test_vectors_dict[row_index]={}\n",
    "                neighbourhood_test_vectors_dict[row_index][index]=vector[0]\n",
    "                dataset_paired_test.append([index,row_index,neightbour,row_index,\n",
    "                                        row[\"edited_prompt\"][0],row[\"neighborhood_prompts_low_sim\"][index],2,0])\n",
    "      \n",
    "    return openai_vectors_dict, edit_vectors_dict, neighbourhood_train_vectors_dict, neighbourhood_test_vectors_dict, paraphrase_train_vectors_dict, paraphrase_test_vectors_dict, dataset_paired_train, dataset_paired_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the dataloaders\n",
    "import json,linecache\n",
    "def read_dataset_reduced(file_path_read_dataset: str,data_size):\n",
    "    dataset=[]\n",
    "    values_list = list(range(1, data_size+1))\n",
    "    for index,number in enumerate(values_list):\n",
    "\n",
    "        try:\n",
    "            data_entry = json.loads(linecache.getline(file_path_read_dataset, number).strip())\n",
    "            dataset.append(data_entry)\n",
    "        except Exception as e:\n",
    "            print(index)\n",
    "            print(e)\n",
    "    return dataset\n",
    "file_path_dataset=\"counterfact_test_2_lama_merged.jsonl\"\n",
    "num_samples=4999\n",
    "dataset=read_dataset_reduced(file_path_dataset,data_size=num_samples) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output vector length: 4096\n"
     ]
    }
   ],
   "source": [
    "openai_vectors_dict, edit_vectors_dict, neighbourhood_train_vectors_dict, neighbourhood_test_vectors_dict, paraphrase_train_vectors_dict, paraphrase_test_vectors_dict, dataset_paired_train, dataset_paired_test=create_dataset_pairs(dataset,neightbour_control=0)\n",
    "input_dim = len( edit_vectors_dict[0])  \n",
    "print(f\"output vector length: {input_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['orignal_prompt', 'edited_prompt', 'edited_prompt_paraphrases_processed', 'edited_prompt_paraphrases_unprocessed', 'edited_prompt_paraphrases_processed_testing', 'edited_prompt_paraphrases_unprocessed_testing', 'neighborhood_prompts_high_sim', 'neighborhood_prompts_low_sim', 'vector_edited_prompt', 'vector_edited_prompt_paraphrases_processed', 'vector_edited_prompt_paraphrases_processed_testing', 'vectors_neighborhood_prompts_high_sim', 'vectors_neighborhood_prompts_low_sim', 'openai_usable_paraphrases', 'openai_notused_paraphrases', 'openai_usable_paraphrases_embeddings'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_paired_train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_data_loader(dataset_paired_train, openai_vectors_dict, edit_vectors_dict, neighbourhood_train_vectors_dict,paraphrase_train_vectors_dict, batch_size=1, shuffle=True)\n",
    "test_loader = get_data_loader(dataset_paired_test, openai_vectors_dict, edit_vectors_dict, neighbourhood_test_vectors_dict,paraphrase_test_vectors_dict, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0027,  0.0107,  0.0083,  ...,  0.0053, -0.0154,  0.0047]])\n",
      "tensor([[-0.0021,  0.0145, -0.0019,  ...,  0.0054, -0.0233,  0.0023]])\n",
      "tensor([0.])\n",
      "tensor([4164])\n",
      "('GAM-87 Skybolt, created by',)\n",
      "('AIR-2 Genie, created by',)\n",
      "tensor([2])\n",
      "tensor([0])\n",
      "tensor([4164])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    for value in batch:\n",
    "        print(value)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ContrastiveNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(ContrastiveNetwork, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.feature(input1)\n",
    "        output2 = self.feature(input2) \n",
    "        return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        distance = F.pairwise_distance(output1, output2) \n",
    "        loss = label * distance.pow(2) + (1 - label) * F.relu(self.margin - distance).pow(2)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_data_loader(dataset_paired_train, openai_vectors_dict, edit_vectors_dict, neighbourhood_train_vectors_dict,paraphrase_train_vectors_dict, batch_size=16, shuffle=True)\n",
    "test_loader = get_data_loader(dataset_paired_test, openai_vectors_dict, edit_vectors_dict, neighbourhood_test_vectors_dict,paraphrase_test_vectors_dict, batch_size=16, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1017.5143\n",
      "Epoch [2/20], Loss: 514.2677\n",
      "Epoch [3/20], Loss: 374.3015\n",
      "Epoch [4/20], Loss: 303.1574\n",
      "Epoch [5/20], Loss: 260.9980\n",
      "Epoch [6/20], Loss: 232.8871\n",
      "Epoch [7/20], Loss: 212.6188\n",
      "Epoch [8/20], Loss: 197.5293\n",
      "Epoch [9/20], Loss: 185.3619\n",
      "Epoch [10/20], Loss: 175.6341\n",
      "Epoch [11/20], Loss: 167.7147\n",
      "Epoch [12/20], Loss: 160.9575\n",
      "Epoch [13/20], Loss: 155.2666\n",
      "Epoch [14/20], Loss: 150.3882\n",
      "Epoch [15/20], Loss: 146.1731\n",
      "Epoch [16/20], Loss: 142.4765\n",
      "Epoch [17/20], Loss: 139.2749\n",
      "Epoch [18/20], Loss: 136.3055\n",
      "Epoch [19/20], Loss: 133.7639\n",
      "Epoch [20/20], Loss: 131.4333\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_dim = 4096\n",
    "hidden_dim = 1900\n",
    "model = ContrastiveNetwork(input_dim, hidden_dim).to(device)\n",
    "\n",
    "\n",
    "criterion = ContrastiveLoss(margin=1.5)\n",
    "num_epochs = 20\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, steps_per_epoch=len(train_loader), epochs=num_epochs)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for emb1, emb2, label, _, _, _, _, _, _, _ in train_loader:\n",
    "        emb1, emb2, label = emb1.to(device), emb2.to(device), label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model(emb1, emb2)\n",
    "        loss = criterion(output1, output2, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "torch.save(model, 'Contrastive_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold saved for 4999 edit vectors\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "threshold_map = defaultdict(list)\n",
    "\n",
    "def compute_threshold(model, edit_vector, paraphrase_vector):\n",
    "    \n",
    "    emb_edit, emb_para = model(edit_vector, paraphrase_vector)\n",
    "    dist = torch.dist(emb_edit, emb_para).item()\n",
    "    return dist\n",
    "\n",
    "\n",
    "\n",
    "threshold_map = defaultdict(list)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for emb_edit, emb_para, _, _, _, _, _, _, row_index, _ in train_loader:\n",
    "        emb_edit = emb_edit.to(device)\n",
    "        emb_para = emb_para.to(device)\n",
    "\n",
    "\n",
    "        for i in range(len(row_index)):\n",
    "            idx = int(row_index[i])\n",
    "            threshold = compute_threshold(model, emb_edit[i], emb_para[i])\n",
    "\n",
    "\n",
    "            threshold_map[idx].append(threshold)\n",
    "\n",
    "\n",
    "final_threshold_map = {str(k): sum(v) / len(v) for k, v in threshold_map.items()}\n",
    "\n",
    "with open(\"Contrastive_threshold_map.json\", \"w\") as f:\n",
    "    json.dump(final_threshold_map, f, indent=4)\n",
    "\n",
    "print(f\"Threshold saved for {len(final_threshold_map)} edit vectors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_number: 4973 and pos_numebr: 4999\n",
      "loc_number: 19283 and loc_numebr: 24995\n",
      "Generalization: 0.9948\n",
      "Locality: 0.7715\n",
      "Incorrect predictions saved to 'incorrect_predictions.json'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_label(model, emb1, emb2, label, threshold, generalization, locality, positive_total, negative_total):\n",
    "    emb_1, emb_2 = model(emb1, emb2)\n",
    "\n",
    "    distance = torch.dist(emb_1, emb_2).item()\n",
    "\n",
    "    if label == 1:\n",
    "        positive_total += 1\n",
    "        if distance < threshold:\n",
    "            generalization += 1\n",
    "            return 1, distance, generalization, locality, positive_total, negative_total\n",
    "        else:\n",
    "            return 0, distance, generalization, locality, positive_total, negative_total\n",
    "        \n",
    "    if label == 0:\n",
    "        negative_total += 1\n",
    "        if distance > threshold:\n",
    "            locality += 1\n",
    "            return 0, distance, generalization, locality, positive_total, negative_total\n",
    "        else:\n",
    "            return 1, distance, generalization, locality, positive_total, negative_total\n",
    "\n",
    "with open(\"Contrastive_threshold_map.json\", \"r\") as f:\n",
    "    threshold_map = json.load(f)\n",
    "\n",
    "positive_total = 0\n",
    "negative_total = 0\n",
    "correct = 0\n",
    "generalization_number = 0\n",
    "locality_number = 0\n",
    "incorrect_predictions = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for emb1, emb2, label, _, sent1, sent2, _, _, row_index, _ in test_loader:\n",
    "        emb1 = emb1.to(device)\n",
    "        emb2 = emb2.to(device)\n",
    "        row_index = row_index.cpu().numpy()\n",
    "\n",
    "        for i in range(len(row_index)):\n",
    "            threshold = threshold_map.get(str(row_index[i]), 1.0)\n",
    "            single_label = label[i].item()\n",
    "\n",
    "            predicted_label, distance, generalization_number, locality_number, positive_total, negative_total = predict_label(model, emb1[i], emb2[i], single_label, threshold, generalization_number, locality_number, positive_total, negative_total)\n",
    "\n",
    "            if predicted_label == label[i]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect_predictions.append({\n",
    "                    \"row_index\": row_index[i],\n",
    "                    \"true_label\": int(label[i]),\n",
    "                    \"predicted_label\": predicted_label,\n",
    "                    \"edit_sentence\": sent1[i], \n",
    "                    \"paraphrase_sentence\": sent2[i],  \n",
    "                    \"similarity\": distance,    \n",
    "                    \"threshold\": threshold     \n",
    "                        \n",
    "                })\n",
    "print(f\"gen_number: {generalization_number} and pos_numebr: {positive_total}\")\n",
    "print(f\"loc_number: {locality_number} and loc_numebr: {negative_total}\")\n",
    "\n",
    "generalization = generalization_number / positive_total\n",
    "locality = locality_number / negative_total\n",
    "print(f\"Generalization: {generalization:.4f}\")\n",
    "print(f\"Locality: {locality:.4f}\")\n",
    "\n",
    "\n",
    "with open(\"Contrastive_incorrect_predictions.json\", \"w\") as f:\n",
    "    json.dump(incorrect_predictions, f, indent=4, default=str)\n",
    "\n",
    "\n",
    "print(f\"Incorrect predictions saved to 'incorrect_predictions.json'\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
